{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence Learning -English-Hindi Translation data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMt5AIAJ0DqZUpK3Xl4ZS95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiranraou/Python-Projects/blob/main/Sequence_Learning_English_Hindi_Translation_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SD37VFTSz0r"
      },
      "source": [
        "**Sequence Learning - Assignment 3**\n",
        "\n",
        "**Problem Statement 2**\n",
        "\n",
        "**Machine Translation is one of the widely used use-cases for the \n",
        "sequence to sequence learning models.\n",
        "Using existing English-Hindi Translation data, build an encoder\u0002decoder model to predict Hindi sentences from a given English \n",
        "sentence.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siCNmLMDTdbL"
      },
      "source": [
        "**Dataset Description.**\n",
        "\n",
        "Dataset contains two major columns - english_sentence & hindi_sentence,which are given translations. There \n",
        "are many sources available for this data - but you have to use \"ted\" for this assignment.\n",
        "\n",
        "Dataset downloaded in google drive from below details\n",
        "\n",
        "**!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 \n",
        "(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \n",
        "Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: \n",
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\n",
        ",application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" -\n",
        "-header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle\u0002data-sets/200079/441417/bundle/archive.zip?GoogleAccessId=web-data@kaggle\u0002161607.iam.gserviceaccount.com&Expires=1589739508&Signature=oCMrIr4qYblbSyR28S%2F5pDf\n",
        "V1hBlxa901ghZ8bKmasWC9msw%2BHmyAooohSK4f0y2hVLU9pmhnEq7%2FZ3ncWZJTjWd4NK0Hhygpk43fkAv\n",
        "pvNhTQcAiExtojT%2FRfXrRR6ZR%2FzEyqH1nh1ywqcLnTqJRwjqzV0PbCgnmNIczOO533FVgkZJwZk59kNwJ\n",
        "uIOU98NIA1zhSxd0q%2ByTGDATwNFNmYalISRyCCFlUtsyjP%2Fk8zUHeQ2gU5lFGLyQeE7584D1uzD6klWV6\n",
        "Ng%2BhJzIWaq3laUZNOuoD7Sm4dxy2t4Ip%2Fc%2BPLIC5ZdZHU%2F9I8LGsbfTJUcdujy72%2BN1hBo2Ts3r\n",
        "w%3D%3D&response-content-disposition=attachment%3B+filename%3Dhindienglish\u0002corpora.zip\" -c -O 'hindienglish-corpora.zip*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JmmqsjFSmUJ"
      },
      "source": [
        "**Mounting Google Drive.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-K-SEti3sW-",
        "outputId": "9861c494-c04b-45bd-8958-2142a01e87fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTHCE04-Nj0Z"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TYOXHNY4bGL"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "tf.executing_eagerly()\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "        \n",
        "PATH = \"/content/drive/MyDrive/NIT Warangal _Industry Project/Hindi_English_Truncated_Corpus.csv\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CNb4-BZN7Zn"
      },
      "source": [
        "**Preprocess English and Hindi sentences.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Je9W2J7RLl"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "\n",
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVw-A9qz7Rci"
      },
      "source": [
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    lines = lines[lines['source']=='ted']\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJf-fTUB7V7i"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQmuoTs8OFhD"
      },
      "source": [
        "**Tokenization of the data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o190IE3V7V_-"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cpa12Dz7WF-"
      },
      "source": [
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMcTFWFM7qVR"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrMSdRm6ORJh"
      },
      "source": [
        "**Create Train and Test dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YpWxIDG7qZY",
        "outputId": "d6a5f09d-2775-4587-922f-b8285b02f1be"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31904 31904 7977 7977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CmalgOb7qes",
        "outputId": "f66f5dfc-8e65-466f-f4ca-d39e0cd163a8"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "5 ----> to\n",
            "269 ----> ask\n",
            "71 ----> them\n",
            "5 ----> to\n",
            "215 ----> always\n",
            "19 ----> have\n",
            "3 ----> the\n",
            "139 ----> right\n",
            "569 ----> answer\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "77 ----> उन्हें\n",
            "246 ----> हमेशा\n",
            "165 ----> केवल\n",
            "189 ----> सही\n",
            "628 ----> उत्तर\n",
            "255 ----> देने\n",
            "4 ----> के\n",
            "118 ----> लिये\n",
            "2245 ----> प्रोत्साहित\n",
            "47 ----> करने\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXpj7HOT7Rgy"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PvcKStlO54-"
      },
      "source": [
        "Encoder Decoder with Attention Model\n",
        "\n",
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEGj6tf78GJF"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8yeRNT8Pg-I"
      },
      "source": [
        "**Attention Mechanism**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-XCp5gj8GNE"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZAAEo1MQfEN"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFgFUUni8GTI"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nxlRJPFQnAs"
      },
      "source": [
        "**Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRyUWMWQ8GYK"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jovVz27r8GcN"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgAWsCyXQ3WQ"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkI3bKhn8rG2"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))      \n",
        "  return batch_loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynCKiGF08rL0",
        "outputId": "caf52ad8-c9de-4df0-8048-11f272ade3c0"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.7969\n",
            "Epoch 1 Batch 100 Loss 1.9372\n",
            "Epoch 1 Batch 200 Loss 2.0015\n",
            "Epoch 1 Batch 300 Loss 1.9136\n",
            "Epoch 1 Batch 400 Loss 1.9209\n",
            "Epoch 1 Loss 1.9469\n",
            "Time taken for 1 epoch 204.45168614387512 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6501\n",
            "Epoch 2 Batch 100 Loss 1.8804\n",
            "Epoch 2 Batch 200 Loss 1.5748\n",
            "Epoch 2 Batch 300 Loss 1.5459\n",
            "Epoch 2 Batch 400 Loss 1.6075\n",
            "Epoch 2 Loss 1.7396\n",
            "Time taken for 1 epoch 154.15194725990295 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6908\n",
            "Epoch 3 Batch 100 Loss 1.5453\n",
            "Epoch 3 Batch 200 Loss 1.5844\n",
            "Epoch 3 Batch 300 Loss 1.6004\n",
            "Epoch 3 Batch 400 Loss 1.5142\n",
            "Epoch 3 Loss 1.6322\n",
            "Time taken for 1 epoch 154.0784261226654 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4988\n",
            "Epoch 4 Batch 100 Loss 1.4972\n",
            "Epoch 4 Batch 200 Loss 1.5666\n",
            "Epoch 4 Batch 300 Loss 1.4768\n",
            "Epoch 4 Batch 400 Loss 1.4433\n",
            "Epoch 4 Loss 1.5411\n",
            "Time taken for 1 epoch 154.55977177619934 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4095\n",
            "Epoch 5 Batch 100 Loss 1.4374\n",
            "Epoch 5 Batch 200 Loss 1.4059\n",
            "Epoch 5 Batch 300 Loss 1.4108\n",
            "Epoch 5 Batch 400 Loss 1.3250\n",
            "Epoch 5 Loss 1.4458\n",
            "Time taken for 1 epoch 153.99459290504456 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3561\n",
            "Epoch 6 Batch 100 Loss 1.2532\n",
            "Epoch 6 Batch 200 Loss 1.3663\n",
            "Epoch 6 Batch 300 Loss 1.3512\n",
            "Epoch 6 Batch 400 Loss 1.3468\n",
            "Epoch 6 Loss 1.3481\n",
            "Time taken for 1 epoch 154.2616081237793 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2133\n",
            "Epoch 7 Batch 100 Loss 1.2067\n",
            "Epoch 7 Batch 200 Loss 1.2875\n",
            "Epoch 7 Batch 300 Loss 1.1713\n",
            "Epoch 7 Batch 400 Loss 1.1932\n",
            "Epoch 7 Loss 1.2550\n",
            "Time taken for 1 epoch 153.75125741958618 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2712\n",
            "Epoch 8 Batch 100 Loss 1.0748\n",
            "Epoch 8 Batch 200 Loss 1.2236\n",
            "Epoch 8 Batch 300 Loss 1.1743\n",
            "Epoch 8 Batch 400 Loss 1.2689\n",
            "Epoch 8 Loss 1.1655\n",
            "Time taken for 1 epoch 154.14900422096252 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9900\n",
            "Epoch 9 Batch 100 Loss 1.1137\n",
            "Epoch 9 Batch 200 Loss 1.0196\n",
            "Epoch 9 Batch 300 Loss 1.1116\n",
            "Epoch 9 Batch 400 Loss 1.0322\n",
            "Epoch 9 Loss 1.0790\n",
            "Time taken for 1 epoch 153.77461338043213 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8920\n",
            "Epoch 10 Batch 100 Loss 0.9311\n",
            "Epoch 10 Batch 200 Loss 0.9057\n",
            "Epoch 10 Batch 300 Loss 0.9991\n",
            "Epoch 10 Batch 400 Loss 0.9756\n",
            "Epoch 10 Loss 0.9969\n",
            "Time taken for 1 epoch 154.42092180252075 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8326\n",
            "Epoch 11 Batch 100 Loss 0.8766\n",
            "Epoch 11 Batch 200 Loss 0.9378\n",
            "Epoch 11 Batch 300 Loss 0.9093\n",
            "Epoch 11 Batch 400 Loss 0.8906\n",
            "Epoch 11 Loss 0.9190\n",
            "Time taken for 1 epoch 153.7243778705597 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.8276\n",
            "Epoch 12 Batch 100 Loss 0.7520\n",
            "Epoch 12 Batch 200 Loss 0.7745\n",
            "Epoch 12 Batch 300 Loss 0.9201\n",
            "Epoch 12 Batch 400 Loss 0.7443\n",
            "Epoch 12 Loss 0.8462\n",
            "Time taken for 1 epoch 154.27123355865479 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7953\n",
            "Epoch 13 Batch 100 Loss 0.7739\n",
            "Epoch 13 Batch 200 Loss 0.7645\n",
            "Epoch 13 Batch 300 Loss 0.7056\n",
            "Epoch 13 Batch 400 Loss 0.7108\n",
            "Epoch 13 Loss 0.7794\n",
            "Time taken for 1 epoch 153.98810243606567 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.7188\n",
            "Epoch 14 Batch 100 Loss 0.6848\n",
            "Epoch 14 Batch 200 Loss 0.6123\n",
            "Epoch 14 Batch 300 Loss 0.6818\n",
            "Epoch 14 Batch 400 Loss 0.6760\n",
            "Epoch 14 Loss 0.7194\n",
            "Time taken for 1 epoch 154.2171025276184 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.6219\n",
            "Epoch 15 Batch 100 Loss 0.6356\n",
            "Epoch 15 Batch 200 Loss 0.6495\n",
            "Epoch 15 Batch 300 Loss 0.6304\n",
            "Epoch 15 Batch 400 Loss 0.5999\n",
            "Epoch 15 Loss 0.6653\n",
            "Time taken for 1 epoch 153.8975327014923 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.6088\n",
            "Epoch 16 Batch 100 Loss 0.5621\n",
            "Epoch 16 Batch 200 Loss 0.5794\n",
            "Epoch 16 Batch 300 Loss 0.5809\n",
            "Epoch 16 Batch 400 Loss 0.6004\n",
            "Epoch 16 Loss 0.6161\n",
            "Time taken for 1 epoch 154.33302640914917 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5901\n",
            "Epoch 17 Batch 100 Loss 0.5822\n",
            "Epoch 17 Batch 200 Loss 0.5266\n",
            "Epoch 17 Batch 300 Loss 0.6051\n",
            "Epoch 17 Batch 400 Loss 0.6469\n",
            "Epoch 17 Loss 0.5705\n",
            "Time taken for 1 epoch 153.68148922920227 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.4663\n",
            "Epoch 18 Batch 100 Loss 0.5028\n",
            "Epoch 18 Batch 200 Loss 0.5108\n",
            "Epoch 18 Batch 300 Loss 0.4970\n",
            "Epoch 18 Batch 400 Loss 0.5239\n",
            "Epoch 18 Loss 0.5308\n",
            "Time taken for 1 epoch 154.05421376228333 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.4325\n",
            "Epoch 19 Batch 100 Loss 0.4935\n",
            "Epoch 19 Batch 200 Loss 0.4131\n",
            "Epoch 19 Batch 300 Loss 0.5442\n",
            "Epoch 19 Batch 400 Loss 0.4645\n",
            "Epoch 19 Loss 0.4901\n",
            "Time taken for 1 epoch 153.62936854362488 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4609\n",
            "Epoch 20 Batch 100 Loss 0.4268\n",
            "Epoch 20 Batch 200 Loss 0.5409\n",
            "Epoch 20 Batch 300 Loss 0.4440\n",
            "Epoch 20 Batch 400 Loss 0.4253\n",
            "Epoch 20 Loss 0.4587\n",
            "Time taken for 1 epoch 153.70083665847778 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCfTRr528rRJ"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC9iNBHn8rW6"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZuYeHx8Ggo",
        "outputId": "bdf4c511-878a-4f53-ad03-4f0a887fffa2"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f87e7447a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6xMAO4C7Rmc",
        "outputId": "4b0d609b-b2f4-4d25-961b-df9d9adbe164"
      },
      "source": [
        "translate(u'politicians do not have permission to do what needs to be done.')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: politicians do not have permission to do what needs to be done .\n",
            "Predicted translation: जो कि कैसे उपलब्ध नहीं है जो कि पालन करना है जो कि पालन करना है जो कि पालन करना है जो कि पालन करना है जो कि पालन करना है जो कि \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}